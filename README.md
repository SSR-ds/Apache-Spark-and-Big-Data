# Apache-Spark-and-Big-Data

1) Using DataBricks Community Edition to analyze data and work on the data and to practice Spark. Here we are cleaning the data and removing the null values and pushing them into permanent view Arvato format to perform Exploratory Data Analysis on the dataset.

2) Exploratory Analysis Using Spark and Python. DataBricks gives us various methods to visualize the data through simple commands

3) While Java is the basis for Spark we are installing JVM version 8 or higher to make spark run on google colab. Further, then we install the type of apche spark that we need from the official site. The we install pyarrow to run pandas commands directly on the spark dataframe. Then we check it with a dataset loading through spark commands. findspark package helps in setting up system.path variables so that spark can be initialized.
